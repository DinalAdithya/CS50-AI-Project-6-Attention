# Transformer Attention Visualization with BERT

## Overview
This project visualizes the attention mechanisms in transformer models using BERT. It demonstrates how attention weights are computed and visualized for masked language modeling tasks, providing insights into model behavior and token relationships during text processing.

## Features
- Uses BERT for masked language modeling tasks.
- Visualizes attention weights during token processing.
- Generates heatmap-style diagrams to illustrate token relationships.
- Easily extendable to other transformer-based models.

## Requirements
- Python 3.x
- TensorFlow
- Hugging Face Transformers
- Matplotlib
- PIL (Python Imaging Library)
- NumPy

## Installation
To install the necessary dependencies, run:


